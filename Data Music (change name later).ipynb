{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre is a Construct Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we loaded the data in, and made sure to convert all qualitative data to quantitative so we could use them in model building. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40546, 13)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "np.random.seed(42)\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "from csv import reader\n",
    "\n",
    "\n",
    "with open('cs305fp_music_genre.csv', 'r') as read_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    count = 0\n",
    "    data = []\n",
    "    titles = []\n",
    "    artists = []\n",
    "    instance_id = []\n",
    "    genres = []\n",
    "    musicKeys = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        if count == 0:\n",
    "            originalcol = row\n",
    "            columns = row[3:14]+row[16:]\n",
    "        else:\n",
    "            instance_id.append(row[0])\n",
    "            artists.append(row[1])\n",
    "            titles.append(row[2])\n",
    "            \n",
    "            #music key (9 in list)\n",
    "            if row[9] in musicKeys:\n",
    "                row[9] = musicKeys.index(row[9])\n",
    "            \n",
    "            #mode (12 in list)\n",
    "            if row[12] == 'Major':\n",
    "                row[12] = 1\n",
    "            elif row[12] == 'Minor':\n",
    "                row[12] = 0\n",
    "            \n",
    "            \n",
    "            #tempo (14 in list) \n",
    "            if row[14] == \"?\":\n",
    "                row[14] = ''\n",
    "            \n",
    "            #music_genre (17 in list)\n",
    "            if row[17] in genres and row[17] != 0:\n",
    "                row[17] = genres.index(row[17])\n",
    "            else:\n",
    "                genres.append(row[17])\n",
    "                row[17] = genres.index(row[17])\n",
    "            \n",
    "            #make sure to not include list items 0-2 and item 14,15 in list\n",
    "            new = row[3:14]+row[16:]\n",
    "            if row[17] != 0 and row[6] != '-1.0':\n",
    "                data.append(new)\n",
    "            \n",
    "            \n",
    "            \n",
    "        count = count + 1\n",
    "        \n",
    "        \n",
    "#Cleaning the data to chance all spaces to zeros and floats\n",
    "total = 0\n",
    "for count,row in enumerate(data):\n",
    "    for count2,c in enumerate(row):\n",
    "        if data[count][count2] != '':\n",
    "            data[count][count2] = float(data[count][count2])\n",
    "        else:\n",
    "            total = total + 1\n",
    "            #print(count,count2) #see which areas have missingness\n",
    "            data[count][count2] = 0\n",
    "            \n",
    "#From the code above, we found that tempo has 4081 pieces of missingness, so we removed the column from the dataset\n",
    "#as to eliminate bias\n",
    "\n",
    "data = data[:-3] #remove NA rows \n",
    "\n",
    "#Change to np.array to make it easier to work with            \n",
    "data = np.array(data)\n",
    " \n",
    "#shape of our uploaded dataset\n",
    "print(np.shape(data))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['popularity', 'acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'speechiness', 'valence', 'music_genre']\n"
     ]
    }
   ],
   "source": [
    "print(columns) #column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Electronic', 'Anime', '', 'Jazz', 'Alternative', 'Country', 'Rap', 'Blues', 'Rock', 'Classical', 'Hip-Hop']\n"
     ]
    }
   ],
   "source": [
    "print(genres) #all our genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and More Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur = df.iloc[:,6]\n",
    "for i in range(len(dur)):\n",
    "    if dur[i] < 0.0:\n",
    "        dur[i] = 0.00\n",
    "        \n",
    "#finding the average of the duration of the songs (in milliseconds)\n",
    "avg = np.sum(dur) / len(dur)\n",
    "avg\n",
    "\n",
    "#replacing all 0.0 values\n",
    "for i in range(len(dur)):\n",
    "    if dur[i] == 0.0:\n",
    "        dur[i] = avg\n",
    "        \n",
    "df.iloc[:,6] = dur\n",
    "df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the Data \n",
    "DATA = np.array(df)\n",
    "X = DATA[:,:-1]\n",
    "y = DATA[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we build a Random Forest and kNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest\n",
      "0.55154130702836\n",
      "kNN\n",
      "0.4908754623921085\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import linear_model  # Using sklearn Perceptron classifier\n",
    "from sklearn import tree  # Using Tree classifier\n",
    "from sklearn import neighbors  # Using nearest neighbors classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.svm as svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import ensemble\n",
    "\n",
    "learners = {\n",
    "            'Forest': ensemble.RandomForestClassifier(),\n",
    "            'kNN': neighbors.KNeighborsClassifier(),\n",
    "           }\n",
    "    \n",
    "\n",
    "for classM in learners:\n",
    "    print(classM)\n",
    "    learners[classM].fit(X_train,y_train)\n",
    "    print(learners[classM].score(X_test,y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at the individual one versus all logistic and SVM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.  4.  3. ...  8.  7.  1.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "Logistic\n",
      "[[-0.00411209  0.04006726  0.01131111 -0.00948009  0.03604135  0.00645661\n",
      "   0.01862778  0.00033903 -0.02193301  0.00405736  0.00460544  0.03198464]]\n",
      "0.8893958076448829\n",
      "[10.  4.  3. ...  8.  7.  1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Logistic\n",
      "[[-0.39279531  0.69514429  0.28185     0.12273654 -0.84741803  0.24957125\n",
      "  -0.44174829  0.0803512   0.75570489  0.74670504 -0.66705132 -0.09732633]]\n",
      "0.9998766954377312\n",
      "[10.  4.  3. ...  8.  7.  1.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "Logistic\n",
      "[[ 0.00411679  0.0029762   0.03495118  0.0167104   0.05501808 -0.0103698\n",
      "  -0.00436803 -0.00912021 -0.04844983  0.00745208 -0.00264344  0.01110715]]\n",
      "0.8859432799013564\n",
      "[10.  4.  3. ...  8.  7.  1.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "Logistic\n",
      "[[ 0.00690969 -0.0098482  -0.00739023 -0.02116059 -0.01873358  0.0148223\n",
      "   0.00292883 -0.01055916  0.0568025   0.01114619 -0.00845396 -0.02366428]]\n",
      "0.8882860665844636\n",
      "[10.  4.  3. ...  8.  7.  1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Logistic\n",
      "[[-0.01092986  0.0436281   0.00334917  0.00097462 -0.04208883 -0.00301419\n",
      "  -0.01507816 -0.01171516  0.07442761  0.02500337  0.00658912  0.01105845]]\n",
      "0.8869297163995068\n",
      "[10.  4.  3. ...  8.  7.  1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Logistic\n",
      "[[-0.02404822 -0.06217134 -0.04839659  0.0293805  -0.0155218  -0.01668637\n",
      "   0.00703689  0.02608117 -0.02773298  0.02177003  0.02520699 -0.01055014]]\n",
      "0.8909987669543773\n",
      "[10.  4.  3. ...  8.  7.  1.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "Logistic\n",
      "[[-0.00020161  0.02253646 -0.0276568  -0.01017407  0.03085977  0.01619006\n",
      "  -0.04072434  0.0171663   0.01694209 -0.02511002  0.00420002 -0.02975218]]\n",
      "0.8849568434032059\n",
      "[10.  4.  3. ...  8.  7.  1.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "Logistic\n",
      "[[-0.00419741 -0.01087766  0.02951875  0.012992    0.02074479  0.01223196\n",
      "   0.0110422  -0.00236477 -0.01026401 -0.00691519 -0.01691594 -0.01890152]]\n",
      "0.8905055487053021\n",
      "[10.  4.  3. ...  8.  7.  1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Logistic\n",
      "[[ 0.02565766 -0.03869698 -0.00788581 -0.01058367 -0.01949663 -0.00792196\n",
      "   0.03877068 -0.00397412 -0.04757563 -0.01606088 -0.02631177  0.0195615 ]]\n",
      "0.8879161528976572\n",
      "[10.  4.  3. ...  8.  7.  1.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "Logistic\n",
      "[[ 0.00834236  0.00969243  0.01235957 -0.01269639 -0.04363142 -0.01295094\n",
      "  -0.01752463 -0.00792866  0.00609956 -0.02128801  0.01192952  0.00822934]]\n",
      "0.8951911220715166\n"
     ]
    }
   ],
   "source": [
    "y_train1 = np.array(y_train,copy=True) #create a temporary y (the genres)\n",
    "y_test1 = np.array(y_test,copy=True) #create a temporary y (the genres)\n",
    "\n",
    "learners = {\n",
    "            'Logistic': LogisticRegression()\n",
    "           }\n",
    "\n",
    "\n",
    "count = 0\n",
    "for count in range(10): #for every genre\n",
    "    count = count + 1\n",
    "    for c,i in enumerate(y_train): #for every genre, we will do a one versus rest comparison and change the values to be 0 or 1\n",
    "        if i != float(count):\n",
    "            y_train1[c] = 0.0\n",
    "        else: \n",
    "            y_train1[c] = 1.0\n",
    "\n",
    "    for c1,i1 in enumerate(y_test): #for every genre, we will do a one versus rest comparison and change the values to be 0 or 1\n",
    "        if i1 != float(count):\n",
    "            y_test1[c1] = 0.0\n",
    "        else: \n",
    "            y_test1[c1] = 1.0\n",
    "    print(y_train)\n",
    "    print(y_train1)\n",
    "\n",
    "    #Running Logistic on this genre versus all the rest, and looking at the coefficients for interpretability\n",
    "    for classM in learners:\n",
    "        print(classM)\n",
    "        learners[classM].fit(X_train1,y_train1)\n",
    "        print(learners[classM].coef_)\n",
    "        print(learners[classM].score(X_test1,y_test1))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will combine all of the one versus rest models using the one versus rest classifier to build a new model that can predict specific genres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy : 0.5651048088779285 %\n",
      "\n",
      "\n",
      "Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.71      0.78      0.74       900\n",
      "         3.0       0.59      0.59      0.59       922\n",
      "         4.0       0.50      0.23      0.32       905\n",
      "         5.0       0.51      0.55      0.53       886\n",
      "         6.0       0.43      0.46      0.45       886\n",
      "         7.0       0.56      0.53      0.54       909\n",
      "         8.0       0.48      0.75      0.58       867\n",
      "         9.0       0.82      0.84      0.83       889\n",
      "        10.0       0.47      0.37      0.41       946\n",
      "\n",
      "    accuracy                           0.57      8110\n",
      "   macro avg       0.56      0.57      0.56      8110\n",
      "weighted avg       0.56      0.57      0.55      8110\n",
      "\n",
      "Parameters : \n",
      "{'estimator__C': 1.0, 'estimator__break_ties': False, 'estimator__cache_size': 200, 'estimator__class_weight': None, 'estimator__coef0': 0.0, 'estimator__decision_function_shape': 'ovr', 'estimator__degree': 3, 'estimator__gamma': 'scale', 'estimator__kernel': 'rbf', 'estimator__max_iter': -1, 'estimator__probability': False, 'estimator__random_state': None, 'estimator__shrinking': True, 'estimator__tol': 0.001, 'estimator__verbose': False, 'estimator': SVC(), 'n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "#One versus Rest SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "   \n",
    "# Creating the SVM model\n",
    "model = OneVsRestClassifier(SVC())\n",
    "   \n",
    "# Fitting the model\n",
    "model.fit(X_train, y_train)\n",
    "   \n",
    "# Making a prediction\n",
    "prediction = model.predict(X_test)\n",
    "   \n",
    "# Evaluating the model\n",
    "print(f\"Test Set Accuracy : {accuracy_score(y_test, prediction)} %\\n\\n\")\n",
    "print(f\"Classification Report : \\n\\n{classification_report(y_test, prediction)}\")\n",
    "print(f\"Parameters : \\n{model.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy : 0.5255240443896424 %\n",
      "\n",
      "\n",
      "Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.62      0.70      0.66       900\n",
      "         3.0       0.54      0.45      0.49       922\n",
      "         4.0       0.42      0.31      0.36       905\n",
      "         5.0       0.42      0.57      0.49       886\n",
      "         6.0       0.48      0.32      0.38       886\n",
      "         7.0       0.52      0.41      0.46       909\n",
      "         8.0       0.48      0.68      0.56       867\n",
      "         9.0       0.76      0.80      0.78       889\n",
      "        10.0       0.48      0.51      0.49       946\n",
      "\n",
      "    accuracy                           0.53      8110\n",
      "   macro avg       0.52      0.53      0.52      8110\n",
      "weighted avg       0.52      0.53      0.52      8110\n",
      "\n",
      "{'estimator__C': 1.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__l1_ratio': None, 'estimator__max_iter': 100, 'estimator__multi_class': 'auto', 'estimator__n_jobs': None, 'estimator__penalty': 'l2', 'estimator__random_state': None, 'estimator__solver': 'lbfgs', 'estimator__tol': 0.0001, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': LogisticRegression(), 'n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "#One versus Rest Logistic\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "   \n",
    "# Creating the Logistic model\n",
    "model = OneVsRestClassifier(LogisticRegression())\n",
    "   \n",
    "# Fitting the model \n",
    "model.fit(X_train, y_train)\n",
    "   \n",
    "# Making a prediction\n",
    "prediction = model.predict(X_test)\n",
    "   \n",
    "# Evaluating the model\n",
    "print(f\"Test Set Accuracy : {accuracy_score(y_test, prediction)} %\\n\\n\")\n",
    "print(f\"Classification Report : \\n\\n{classification_report(y_test, prediction)}\")\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy : 0.36189889025893957 \n",
      "\n",
      "\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.58      0.44      0.50       900\n",
      "         3.0       0.30      0.57      0.39       922\n",
      "         4.0       0.21      0.25      0.23       905\n",
      "         5.0       0.45      0.10      0.16       886\n",
      "         6.0       0.31      0.00      0.01       886\n",
      "         7.0       0.30      0.45      0.36       909\n",
      "         8.0       0.46      0.24      0.32       867\n",
      "         9.0       0.81      0.46      0.59       889\n",
      "        10.0       0.32      0.70      0.44       946\n",
      "\n",
      "    accuracy                           0.36      8110\n",
      "   macro avg       0.42      0.36      0.33      8110\n",
      "weighted avg       0.41      0.36      0.33      8110\n",
      "\n",
      "{'estimator__alpha': 0.0001, 'estimator__class_weight': None, 'estimator__early_stopping': False, 'estimator__eta0': 1.0, 'estimator__fit_intercept': True, 'estimator__max_iter': 100, 'estimator__n_iter_no_change': 5, 'estimator__n_jobs': None, 'estimator__penalty': None, 'estimator__random_state': 0, 'estimator__shuffle': True, 'estimator__tol': 0.001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': Perceptron(max_iter=100), 'n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "#One versus Rest Perceptron\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model  # Using sklearn Perceptron classifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "   \n",
    "# Creating the Perceptron model\n",
    "model = OneVsRestClassifier(linear_model.Perceptron(max_iter = 100))\n",
    "   \n",
    "# Fitting the model\n",
    "model.fit(X_train, y_train)\n",
    "   \n",
    "# Making a prediction\n",
    "prediction = model.predict(X_test)\n",
    "   \n",
    "# Evaluating the model\n",
    "print(f\"Test Set Accuracy : {accuracy_score(y_test, prediction)} \\n\\n\")\n",
    "print(f\"Classification Report : \\n{classification_report(y_test, prediction)}\")\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also investigated the model of the one versus one classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy : 0.5901356350184956 %\n",
      "\n",
      "\n",
      "Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.73      0.75       900\n",
      "         3.0       0.63      0.63      0.63       922\n",
      "         4.0       0.47      0.39      0.42       905\n",
      "         5.0       0.55      0.56      0.55       886\n",
      "         6.0       0.46      0.37      0.41       886\n",
      "         7.0       0.64      0.55      0.59       909\n",
      "         8.0       0.49      0.72      0.59       867\n",
      "         9.0       0.85      0.84      0.84       889\n",
      "        10.0       0.48      0.53      0.50       946\n",
      "\n",
      "    accuracy                           0.59      8110\n",
      "   macro avg       0.59      0.59      0.59      8110\n",
      "weighted avg       0.59      0.59      0.59      8110\n",
      "\n",
      "Parameters : \n",
      "{'estimator__C': 1.0, 'estimator__break_ties': False, 'estimator__cache_size': 200, 'estimator__class_weight': None, 'estimator__coef0': 0.0, 'estimator__decision_function_shape': 'ovr', 'estimator__degree': 3, 'estimator__gamma': 'scale', 'estimator__kernel': 'rbf', 'estimator__max_iter': -1, 'estimator__probability': False, 'estimator__random_state': None, 'estimator__shrinking': True, 'estimator__tol': 0.001, 'estimator__verbose': False, 'estimator': SVC(), 'n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "   \n",
    "# Creating the SVM model\n",
    "model = OneVsOneClassifier(SVC())\n",
    "   \n",
    "# Fitting the model with training data\n",
    "model.fit(X_train, y_train)\n",
    "   \n",
    "# Making a prediction on the test set\n",
    "prediction = model.predict(X_test)\n",
    "   \n",
    "# Evaluating the model\n",
    "print(f\"Test Set Accuracy : {accuracy_score(y_test, prediction)} %\\n\\n\")\n",
    "print(f\"Classification Report : \\n\\n{classification_report(y_test, prediction)}\")\n",
    "print(f\"Parameters : \\n{model.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy : 0.54845869297164 %\n",
      "\n",
      "\n",
      "Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.66      0.71      0.69       900\n",
      "         3.0       0.58      0.55      0.57       922\n",
      "         4.0       0.42      0.37      0.40       905\n",
      "         5.0       0.47      0.53      0.50       886\n",
      "         6.0       0.44      0.35      0.39       886\n",
      "         7.0       0.54      0.47      0.50       909\n",
      "         8.0       0.51      0.66      0.57       867\n",
      "         9.0       0.81      0.80      0.81       889\n",
      "        10.0       0.49      0.49      0.49       946\n",
      "\n",
      "    accuracy                           0.55      8110\n",
      "   macro avg       0.55      0.55      0.55      8110\n",
      "weighted avg       0.55      0.55      0.55      8110\n",
      "\n",
      "Parameters : \n",
      "{'estimator__C': 1.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__l1_ratio': None, 'estimator__max_iter': 100, 'estimator__multi_class': 'auto', 'estimator__n_jobs': None, 'estimator__penalty': 'l2', 'estimator__random_state': None, 'estimator__solver': 'lbfgs', 'estimator__tol': 0.0001, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': LogisticRegression(), 'n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "   \n",
    "# Creating the Logistic model\n",
    "model = OneVsOneClassifier(LogisticRegression())\n",
    "   \n",
    "# Fitting the model with training data\n",
    "model.fit(X_train, y_train)\n",
    "   \n",
    "# Making a prediction on the test set\n",
    "prediction = model.predict(X_test)\n",
    "   \n",
    "# Evaluating the model\n",
    "print(f\"Test Set Accuracy : {accuracy_score(y_test, prediction)} %\\n\\n\")\n",
    "print(f\"Classification Report : \\n\\n{classification_report(y_test, prediction)}\")\n",
    "print(f\"Parameters : \\n{model.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
